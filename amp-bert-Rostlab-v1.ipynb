{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, hamming_loss, jaccard_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "from transformers import AutoTokenizer, Trainer, TrainingArguments, BertForSequenceClassification, AdamW\n",
        "\n",
        "# Define a class for the AMP data that will correctly format the sequence information\n",
        "# for fine-tuning with the Huggingface API\n",
        "# The input DataFrame columns must be formatted the same way as the given example\n",
        "\n",
        "class amp_data(Dataset):\n",
        "    def __init__(self, df, tokenizer_name='Rostlab/prot_bert_bfd', max_len=200):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, do_lower_case=False)\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.seqs, self.labels = self.get_seqs_labels(df)\n",
        "\n",
        "    def get_seqs_labels(self, df):\n",
        "        # Isolate the amino acid sequences and their respective AMP labels\n",
        "        seqs = list(df['Sequence'])\n",
        "        labels = list(df[['Antibacterial', 'Antiviral', 'Antiparasitic', 'Antifungal']].values)\n",
        "        labels = torch.tensor(labels, dtype=torch.float32)\n",
        "        return seqs, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = \" \".join(\"\".join(self.seqs[idx].split()))\n",
        "        seq_ids = self.tokenizer(seq, truncation=True, padding='max_length', max_length=self.max_len)\n",
        "\n",
        "        sample = {key: torch.tensor(val) for key, val in seq_ids.items()}\n",
        "        sample['labels'] = torch.tensor(self.labels[idx])\n",
        "\n",
        "        return sample\n",
        "\n",
        "# Read in the train dataset\n",
        "# Create an amp_data class of the dataset\n",
        "\n",
        "data_url = 'https://raw.githubusercontent.com/Kevinzhn/AMP-BERT-Multilabel/main/treinamento'\n",
        "df = pd.read_csv(data_url, index_col=None)  # Use index_col=None to prevent treating \"Numero\" as an index column\n",
        "df = df.sample(frac=1, random_state=0)\n",
        "print(df.head(7))\n",
        "train_dataset = amp_data(df)\n",
        "\n",
        "# Define the necessary metrics for performance evaluation\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions > 0.5  # 假设你的模型输出的是概率或者经过阈值处理的概率结果\n",
        "    hamming_loss_val = hamming_loss(labels, preds)\n",
        "    jaccard_score_val = jaccard_score(labels, preds, average='samples')  # 使用 'samples' 平均指标\n",
        "    return {\n",
        "        'hamming_loss': hamming_loss_val,\n",
        "        'jaccard_score_samples': jaccard_score_val,\n",
        "    }\n",
        "\n",
        "\n",
        "# Define the model initializing function for Trainer in Huggingface\n",
        "\n",
        "def model_init():\n",
        "    return BertForSequenceClassification.from_pretrained('Rostlab/prot_bert_bfd', num_labels=4)\n",
        "\n",
        "# Read in the evaluation dataset\n",
        "eval_data_url = 'https://raw.githubusercontent.com/Kevinzhn/AMP-BERT-Multilabel/main/teste'\n",
        "eval_df = pd.read_csv(eval_data_url, index_col=None)\n",
        "eval_df = eval_df.sample(frac=1, random_state=0)\n",
        "eval_dataset = amp_data(eval_df)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=15,\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=1,\n",
        "    warmup_steps=0,\n",
        "    weight_decay=0.1,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy='epoch',\n",
        "    gradient_accumulation_steps=64,\n",
        "    fp16=True,\n",
        "    fp16_opt_level=\"O2\",\n",
        "    run_name=\"AMP-BERT\",\n",
        "    seed=0,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "id": "eIaHcP_omajm",
        "outputId": "7f9404e0-26e5-44bd-bbb9-223e793e4c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "                                        Sequence  Antibacterial  Antiviral  \\\n",
            "15                                     RWRWWWRVY              1          0   \n",
            "3262                   CNIAPASIVSRNIVYTRAQPNQDIA              0          1   \n",
            "499                       YPGPQAKEDSEGPSQGPASREK              1          0   \n",
            "4132                               FIPLVSGLFSRLL              1          0   \n",
            "4560                       DWTFANWSCLVCDDCSVNLTV              1          0   \n",
            "608               ILQKAVLDCLKAAGSSLSKAAITAIYNKIT              1          0   \n",
            "1362  GALWGAPAGGVGALPGAFVGAHVGAIAGGFACMGGMIGNKFN              1          0   \n",
            "\n",
            "      Antiparasitic  Antifungal  Numero  \n",
            "15                0           0       9  \n",
            "3262              0           0      25  \n",
            "499               0           0      22  \n",
            "4132              0           1      13  \n",
            "4560              0           1      21  \n",
            "608               0           0      30  \n",
            "1362              0           0      42  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-1fbe90007d10>:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  labels = torch.tensor(labels, dtype=torch.float32)\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "<ipython-input-1-1fbe90007d10>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sample['labels'] = torch.tensor(self.labels[idx])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='519' max='1290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 519/1290 1:11:18 < 1:46:20, 0.12 it/s, Epoch 5.99/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Jaccard Score Samples</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "      <th>Steps Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.298975</td>\n",
              "      <td>0.541473</td>\n",
              "      <td>0.502743</td>\n",
              "      <td>28.123300</td>\n",
              "      <td>65.888000</td>\n",
              "      <td>8.249000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.499800</td>\n",
              "      <td>0.346870</td>\n",
              "      <td>0.523102</td>\n",
              "      <td>0.046096</td>\n",
              "      <td>28.101900</td>\n",
              "      <td>65.938000</td>\n",
              "      <td>8.256000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.508900</td>\n",
              "      <td>0.212628</td>\n",
              "      <td>0.452154</td>\n",
              "      <td>0.645305</td>\n",
              "      <td>28.275200</td>\n",
              "      <td>65.535000</td>\n",
              "      <td>8.205000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.446600</td>\n",
              "      <td>0.240826</td>\n",
              "      <td>0.475572</td>\n",
              "      <td>0.554236</td>\n",
              "      <td>28.245500</td>\n",
              "      <td>65.603000</td>\n",
              "      <td>8.214000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.455200</td>\n",
              "      <td>0.248246</td>\n",
              "      <td>0.481494</td>\n",
              "      <td>0.588730</td>\n",
              "      <td>28.093700</td>\n",
              "      <td>65.958000</td>\n",
              "      <td>8.258000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.469300</td>\n",
              "      <td>0.290205</td>\n",
              "      <td>0.475523</td>\n",
              "      <td>0.219194</td>\n",
              "      <td>27.873600</td>\n",
              "      <td>66.479000</td>\n",
              "      <td>8.323000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-1fbe90007d10>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sample['labels'] = torch.tensor(self.labels[idx])\n",
            "<ipython-input-1-1fbe90007d10>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sample['labels'] = torch.tensor(self.labels[idx])\n",
            "<ipython-input-1-1fbe90007d10>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sample['labels'] = torch.tensor(self.labels[idx])\n",
            "<ipython-input-1-1fbe90007d10>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sample['labels'] = torch.tensor(self.labels[idx])\n",
            "<ipython-input-1-1fbe90007d10>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  sample['labels'] = torch.tensor(self.labels[idx])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlyqGrVwidKY",
        "outputId": "df4862e4-dd93-406f-9a81-9693cb876a44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers[torch])\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[torch])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[torch])\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.65.0)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, accelerate\n",
            "Successfully installed accelerate-0.21.0 huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    }
  ]
}
